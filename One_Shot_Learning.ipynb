{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jkh5MSOI7EfT",
        "outputId": "1b86bdaf-e065-4454-d326-e06db04538bc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-pke97o_a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-pke97o_a\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zFDShrjDUTvT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from google.colab import drive\n",
        "import clip\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVKsWyNZUbGA",
        "outputId": "05d31d9d-dc32-4a4f-a6b1-9ca51fcf4f31"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/gdrive/My Drive/data'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izQQtLlS_8xe",
        "outputId": "030ac761-5094-4b11-baf4-74cd4875559b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_random_seed(seed=28):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_random_seed(0)\n",
        "\n",
        "# Load CLIP model and preprocessing pipeline\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ],
      "metadata": {
        "id": "sDlmuQhrNtdh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OneShotDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, one_shot=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Directory containing image folders for each class.\n",
        "            transform (callable, optional): A function/transform to apply to images.\n",
        "            one_shot (bool): Whether to select only one training sample per class.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.one_shot = one_shot\n",
        "        self.data = []\n",
        "        self.classes = []  # List of class names\n",
        "        self.class_to_idx = {}\n",
        "        self._prepare_data()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        \"\"\"Prepare the dataset by iterating through class directories.\"\"\"\n",
        "        self.classes = sorted(os.listdir(self.root_dir))  # Get all class folders\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "\n",
        "        for cls_name in self.classes:\n",
        "            class_path = os.path.join(self.root_dir, cls_name)\n",
        "            images = sorted(os.listdir(class_path))  # All images in the class folder\n",
        "\n",
        "            if self.one_shot:\n",
        "                # Randomly select only one image per class\n",
        "                images = [random.choice(images)]\n",
        "\n",
        "            for img_name in images:\n",
        "                self.data.append((os.path.join(class_path, img_name), self.class_to_idx[cls_name]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "b34ZLTKtOkr1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Random crop and resize\n",
        "    transforms.RandomHorizontalFlip(),  # Horizontal flip\n",
        "    #transforms.RandomRotation(10),\n",
        "    #transforms.ColorJitter(brightness=0.1, contrast=0.1),# Random rotation\n",
        "    preprocess  # Apply CLIP preprocessing\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Random crop and resize\n",
        "    transforms.RandomHorizontalFlip(),  # Horizontal flip\n",
        "    #transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1), # Random rotation\n",
        "    preprocess  # Apply CLIP preprocessing\n",
        "])\n",
        "\n",
        "# Training dataset: one image per class\n",
        "train_dataset = OneShotDataset(root_dir=\"/content/gdrive/My Drive/data/train\", transform=train_transform, one_shot=True)\n",
        "\n",
        "# Testing dataset: all images\n",
        "test_dataset = OneShotDataset(root_dir=\"/content/gdrive/My Drive/data/test\", transform=test_transform, one_shot=False)"
      ],
      "metadata": {
        "id": "q9Jejz-5Onit"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "BMTHjHL0Opzg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPClassifier(nn.Module):\n",
        "    def __init__(self, clip_model, num_classes):\n",
        "        super(CLIPClassifier, self).__init__()\n",
        "        self.clip_model = clip_model\n",
        "        self.dropout = nn.Dropout(0.4)  # Drop 20% neurons\n",
        "        self.fc = nn.Linear(clip_model.visual.output_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            features = self.clip_model.encode_image(x)\n",
        "\n",
        "        features = features.float()\n",
        "        features = self.dropout(features)  # Apply dropout\n",
        "        return self.fc(features)"
      ],
      "metadata": {
        "id": "AZP4JV0vN2uc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_clip_classifier(model, train_loader, test_loader, epochs=20):\n",
        "    # Define optimizer with different learning rates for classifier head and CLIP backbone\n",
        "    optimizer = torch.optim.Adam([\n",
        "        {'params': model.fc.parameters(), 'lr': 1e-3},  # Classifier head\n",
        "        {'params': model.clip_model.visual.parameters(), 'lr': 1e-5}  # CLIP backbone\n",
        "    ], weight_decay=1e-4)  # Add weight decay for regularization\n",
        "\n",
        "    # Define scheduler to reduce learning rate dynamically\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.7)\n",
        "\n",
        "    # Define loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track metrics\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_accuracy = 100 * correct / total\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {total_loss / len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "        # Update learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Evaluate on test set after each epoch\n",
        "        evaluate_clip_classifier(model, test_loader)\n",
        "\n",
        "def evaluate_clip_classifier(model, test_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "uY8QVJH_OCzJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "num_classes = len(train_dataset.classes)\n",
        "model = CLIPClassifier(clip_model, num_classes).to(device)\n",
        "\n",
        "# Unfreeze CLIP backbone for fine-tuning\n",
        "for param in model.clip_model.visual.parameters():\n",
        "    param.requires_grad = True  # Allow gradient updates for the backbone\n",
        "\n",
        "# Train the model\n",
        "train_clip_classifier(model, train_loader, test_loader, epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErDtFwPMOHFq",
        "outputId": "4dbc0dda-184a-4ca1-abde-792022bb26ac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] - Loss: 2.9983, Accuracy: 0.00%\n",
            "Test Accuracy: 12.75%\n",
            "Epoch [2/20] - Loss: 2.6522, Accuracy: 18.75%\n",
            "Test Accuracy: 21.50%\n",
            "Epoch [3/20] - Loss: 2.2469, Accuracy: 50.00%\n",
            "Test Accuracy: 36.00%\n",
            "Epoch [4/20] - Loss: 1.9493, Accuracy: 68.75%\n",
            "Test Accuracy: 49.00%\n",
            "Epoch [5/20] - Loss: 1.6321, Accuracy: 93.75%\n",
            "Test Accuracy: 56.25%\n",
            "Epoch [6/20] - Loss: 1.3895, Accuracy: 100.00%\n",
            "Test Accuracy: 63.25%\n",
            "Epoch [7/20] - Loss: 1.1174, Accuracy: 100.00%\n",
            "Test Accuracy: 65.00%\n",
            "Epoch [8/20] - Loss: 0.9654, Accuracy: 100.00%\n",
            "Test Accuracy: 66.75%\n",
            "Epoch [9/20] - Loss: 0.8616, Accuracy: 100.00%\n",
            "Test Accuracy: 66.25%\n",
            "Epoch [10/20] - Loss: 0.7080, Accuracy: 100.00%\n",
            "Test Accuracy: 67.75%\n",
            "Epoch [11/20] - Loss: 0.7051, Accuracy: 100.00%\n",
            "Test Accuracy: 68.00%\n",
            "Epoch [12/20] - Loss: 0.5766, Accuracy: 100.00%\n",
            "Test Accuracy: 67.75%\n",
            "Epoch [13/20] - Loss: 0.5033, Accuracy: 100.00%\n",
            "Test Accuracy: 67.75%\n",
            "Epoch [14/20] - Loss: 0.4867, Accuracy: 100.00%\n",
            "Test Accuracy: 69.25%\n",
            "Epoch [15/20] - Loss: 0.4041, Accuracy: 100.00%\n",
            "Test Accuracy: 68.00%\n",
            "Epoch [16/20] - Loss: 0.3679, Accuracy: 100.00%\n",
            "Test Accuracy: 67.75%\n",
            "Epoch [17/20] - Loss: 0.3430, Accuracy: 100.00%\n",
            "Test Accuracy: 68.75%\n",
            "Epoch [18/20] - Loss: 0.3454, Accuracy: 100.00%\n",
            "Test Accuracy: 69.25%\n",
            "Epoch [19/20] - Loss: 0.3111, Accuracy: 100.00%\n",
            "Test Accuracy: 69.50%\n",
            "Epoch [20/20] - Loss: 0.2761, Accuracy: 100.00%\n",
            "Test Accuracy: 70.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'OneShotwithCLIP_model_v2.0.pth')"
      ],
      "metadata": {
        "id": "Z0PLjoDxoxuB"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}